#! /usr/bin/python
#
#   The ultimate radar joining script - more robust than
#   the shell scripts that have been accumulating.
#
#   This will combine radar data from one survey, filter by quality,
#   calculate depths, and attach GPS coordinates. All data will be
#   written to an XYZ file in current directory.
#
#   To run this, you need:
#       a set of picking files in 'picking/' generated by icepick
#       a set of rating files in 'rating/' generated by icerate
#       an offset file in 'offsets/' named (PREFIX)_offsets.txt
#
#   Run by invoking with PREFIX and the correct HDF5 dataset as arguments
#

import os
import sys
import math
from irlib import Survey
import traceback

###### DECLARATIONS #######

def read_columns_as_dict(f, delimiter=None):
    """ Read a file object, returning a dictionary with the first column (FIDs)
    as keys. """
    D = {}
    for line in f:
        pp = line.split(delimiter, 1)
        D[pp[0].rjust(16, '0')] = pp[1].split(delimiter)
    return D

def time2depth(t, x, v=1.68e8):
    """ Perform normal moveout and time-to-depth conversion. """
    if x/v < t:
        z = round(math.sqrt((t**2 * v**2/4.0) - (x**2/4.0)), 2)
    else:
        z = 0.0
    return z

def zero_time_correction(x):
    """ Return the airwave travel time over x. """
    return x / 3e8

###### MAIN PROGRAM #######

if len(sys.argv) < 3:
    print """
    SYNTAX: join_radar PREFIX H5FILE

    join_radar combines information from picking, rating, offset, and
    HDF5 files, and computes depths at each valid observation location.
    """
    sys.exit(0)
else:
    prefix = sys.argv[1]
    h5file = sys.argv[2]

qual_min = 2

# Identify the picking files and the rating files
prefix_test = lambda fnm: True if fnm[:len(prefix)] == prefix else False
picking_files = filter(prefix_test, os.listdir('picking'))
rating_files = filter(prefix_test, os.listdir('rating'))
offsets_file = 'offsets/{0}'.format(prefix + '_offsets.txt')
print "Using offsets: ", offsets_file

# Retain the intersection
pull_number = lambda fnm: fnm.rsplit('_', 1)[1].split('.')[0][4:]
picking_lines = map(pull_number, picking_files)
rating_lines = map(pull_number, rating_files)
common_lines = list(set(picking_lines).intersection(rating_lines))

try:
    # For each pair of corresponding files, grab the data
    S = Survey(h5file)
    alldata = {}
    print 'reading from'
    for line in common_lines:

        pickfile = picking_files[picking_lines.index(line)]
        ratefile = rating_files[rating_lines.index(line)]
        print '\t' + pickfile
        with open('picking/' + pickfile) as f:
            P = read_columns_as_dict(f, delimiter=',')
        with open('rating/' + ratefile) as f:
            R = read_columns_as_dict(f)

        # Open radar line to retrieve UTM coords
        L = S.ExtractLine(line)
        sample_rate = L.rate
        traces = [int(s[4:8]) for s in L.metadata.fids]

        for fid in P.keys():
            if fid in R.keys():
                alldata[fid] = [P[fid], R[fid]]         # Picking is field 0
                n = int(fid[4:8])                       # Rating is field 1
                lons = L.metadata.lons[traces.index(n)]
                lats = L.metadata.lats[traces.index(n)]
                alt = L.metadata.alt_asl[traces.index(n)]
                alldata[fid].append((lons,lats,alt))              # UTM is field 2
        del L

    # Get the antenna offsets and add to alldata dict
    with open(offsets_file) as f:
        offsets = read_columns_as_dict(f)
    nmissing = 0
    for fid in offsets.keys():
        if fid in alldata.keys():
            alldata[fid].append(offsets[fid][0])        # Offset is field 3
        else:
            nmissing += 1
    print ("{0} traces in {1} not found in both the rating and picking "
           "files".format(nmissing, offsets_file))

    # Calculate depth and measurement error
    for fid in alldata.keys():
        t = float(alldata[fid][0][2])

        if t != 999.0:

            try:
                t *= sample_rate
                x = float(alldata[fid][3])
                t += zero_time_correction(float(x))
                z = time2depth(t, x)
                alldata[fid].append(z)                  # Depth is field 4
                r = int(alldata[fid][1][0])
                err = (1.0/r**2 * 50.0)
                alldata[fid].append(err)                # Error is field 5

            except IndexError:
                print "\tWarning: {0} missing a field".format(fid)
                print "\t\t", alldata[fid]
                del alldata[fid]

        else:                                           # Remove nodata
            del alldata[fid]

    # Join everything together, filtering by quality
    path_out = "result/"
    if not os.path.exists(path_out):
        os.makedirs(path_out)

    fout = path_out + 'depth_' + prefix + '.xyz'
    print 'writing to ' + fout
    fids = alldata.keys()
    fids.sort()
    with open(fout, 'w') as f:
        f.write('fid\tlongitude\tlatitude\taltitude\tdepth\terror\n')
        for fid in fids:
            if alldata[fid][1][0] >= qual_min:
                f.write('{fidid}\t{lon}\t{lat}\t{alt}\t{depth}\t{err}\n'.format(
                        fidid = fid,
                        lon = alldata[fid][2][0],
                        lat = alldata[fid][2][1],
                        alt = alldata[fid][2][2],
                        depth = alldata[fid][4],
                        err = alldata[fid][5]))

except:
    traceback.print_exc()
